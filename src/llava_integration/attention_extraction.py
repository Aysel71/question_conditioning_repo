# src/llava_integration/attention_extraction.py
"""
LLaVA Integration –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è ground truth attention patterns
"""

import torch
import torch.nn as nn
from typing import Dict, List, Tuple, Optional
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM
from PIL import Image
import json

try:
    from llava.model.builder import load_pretrained_model
    from llava.mm_utils import get_model_name_from_path, tokenizer_image_token
    from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN
    from llava.conversation import conv_templates, SeparatorStyle
    LLAVA_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è  LLaVA not installed. Using mock implementation.")
    LLAVA_AVAILABLE = False


class LLaVAAttentionExtractor:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ attention patterns –∏–∑ LLaVA –º–æ–¥–µ–ª–∏"""
    
    def __init__(self, model_path: str = "liuhaotian/llava-v1.5-7b", device: str = "cuda"):
        self.device = device
        self.model_path = model_path
        
        if LLAVA_AVAILABLE:
            self._load_llava_model()
        else:
            self._create_mock_model()
    
    def _load_llava_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –Ω–∞—Å—Ç–æ—è—â—É—é LLaVA –º–æ–¥–µ–ª—å"""
        try:
            model_name = get_model_name_from_path(self.model_path)
            self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(
                self.model_path, None, model_name, device_map=self.device
            )
            self.model.eval()
            print(f"‚úÖ LLaVA model loaded: {model_name}")
            
        except Exception as e:
            print(f"‚ùå Failed to load LLaVA: {e}")
            self._create_mock_model()
    
    def _create_mock_model(self):
        """–°–æ–∑–¥–∞–µ—Ç mock –º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        print("üîß Using mock LLaVA implementation")
        
        # Mock tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained("microsoft/DialoGPT-medium")
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # Mock model components
        self.model = None
        self.image_processor = None
        self.context_len = 2048
    
    def extract_attention_weights(
        self, 
        image: Image.Image, 
        question: str,
        return_layers: List[int] = [-1]  # –ü–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    ) -> Dict[str, torch.Tensor]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç attention weights –º–µ–∂–¥—É visual –∏ text tokens
        
        Args:
            image: PIL Image
            question: Text question
            return_layers: –°–ª–æ–∏ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è attention
            
        Returns:
            Dictionary —Å attention weights –∏ metadata
        """
        
        if not LLAVA_AVAILABLE or self.model is None:
            return self._mock_attention_extraction(image, question)
        
        try:
            return self._real_attention_extraction(image, question, return_layers)
        except Exception as e:
            print(f"‚ö†Ô∏è  Attention extraction failed: {e}")
            return self._mock_attention_extraction(image, question)
    
    def _real_attention_extraction(
        self, 
        image: Image.Image, 
        question: str,
        return_layers: List[int]
    ) -> Dict[str, torch.Tensor]:
        """–ù–∞—Å—Ç–æ—è—â–µ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ attention –∏–∑ LLaVA"""
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ conversation template
        conv = conv_templates["llava_v1"].copy()
        conv.append_message(conv.roles[0], DEFAULT_IMAGE_TOKEN + "\n" + question)
        conv.append_message(conv.roles[1], None)
        prompt = conv.get_prompt()
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        input_ids = tokenizer_image_token(
            prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt'
        ).unsqueeze(0).to(self.device)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        image_tensor = self.image_processor.preprocess(
            image, return_tensors='pt'
        )['pixel_values'][0].unsqueeze(0).to(self.device)
        
        # Hook –¥–ª—è –ø–µ—Ä–µ—Ö–≤–∞—Ç–∞ attention weights
        attention_weights = {}
        
        def attention_hook(name):
            def hook(module, input, output):
                # output[1] —Å–æ–¥–µ—Ä–∂–∏—Ç attention weights –¥–ª—è MultiHeadAttention
                if len(output) > 1 and output[1] is not None:
                    attention_weights[name] = output[1].detach().cpu()
            return hook
        
        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º hooks –Ω–∞ –Ω—É–∂–Ω—ã—Ö —Å–ª–æ—è—Ö
        handles = []
        for layer_idx in return_layers:
            if layer_idx < 0:
                layer_idx = len(self.model.model.layers) + layer_idx
            
            layer_name = f"layer_{layer_idx}"
            layer = self.model.model.layers[layer_idx]
            
            # Hook –Ω–∞ self-attention
            handle = layer.self_attn.register_forward_hook(attention_hook(layer_name))
            handles.append(handle)
        
        # Forward pass
        with torch.no_grad():
            outputs = self.model.generate(
                input_ids,
                images=image_tensor,
                do_sample=False,
                temperature=0,
                max_new_tokens=1,
                output_attentions=True,
                return_dict_in_generate=True
            )
        
        # –£–¥–∞–ª—è–µ–º hooks
        for handle in handles:
            handle.remove()
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º attention weights
        processed_attention = self._process_attention_weights(
            attention_weights, input_ids, len(self.tokenizer.encode(question))
        )
        
        return processed_attention
    
    def _mock_attention_extraction(
        self, 
        image: Image.Image, 
        question: str
    ) -> Dict[str, torch.Tensor]:
        """Mock implementation –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ fake attention patterns
        num_visual_tokens = 576  # 24x24 patches
        question_tokens = len(question.split()) + 5  # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ
        
        # –°–æ–∑–¥–∞–µ–º attention pattern —Å –Ω–µ–∫–æ—Ç–æ—Ä–æ–π –ª–æ–≥–∏–∫–æ–π
        attention_pattern = torch.zeros(num_visual_tokens)
        
        # –°–∏–º—É–ª–∏—Ä—É–µ–º —Ñ–æ–∫—É—Å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –≤–æ–ø—Ä–æ—Å–∞
        if "color" in question.lower():
            # –§–æ–∫—É—Å –Ω–∞ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã—Ö patches
            center_patches = torch.arange(200, 376)  # –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω–∞—è –æ–±–ª–∞—Å—Ç—å
            attention_pattern[center_patches] = torch.rand(len(center_patches)) * 0.8 + 0.2
        
        elif "how many" in question.lower():
            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ
            attention_pattern = torch.rand(num_visual_tokens) * 0.6 + 0.1
        
        elif "where" in question.lower():
            # –§–æ–∫—É—Å –Ω–∞ –∫—Ä–∞—è—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            edge_patches = list(range(0, 100)) + list(range(476, 576))
            attention_pattern[edge_patches] = torch.rand(len(edge_patches)) * 0.7 + 0.2
        
        else:
            # –û–±—â–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω
            attention_pattern = torch.rand(num_visual_tokens) * 0.5 + 0.1
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        attention_pattern = torch.softmax(attention_pattern, dim=0)
        
        return {
            'visual_attention': attention_pattern,
            'attention_scores': attention_pattern,
            'num_visual_tokens': num_visual_tokens,
            'num_text_tokens': question_tokens,
            'question': question,
            'is_mock': True
        }
    
    def _process_attention_weights(
        self, 
        raw_attention: Dict[str, torch.Tensor],
        input_ids: torch.Tensor,
        question_length: int
    ) -> Dict[str, torch.Tensor]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç raw attention weights"""
        
        processed = {}
        
        for layer_name, attention in raw_attention.items():
            # attention shape: [batch, num_heads, seq_len, seq_len]
            batch_size, num_heads, seq_len, _ = attention.shape
            
            # –£—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ heads
            avg_attention = attention.mean(dim=1)  # [batch, seq_len, seq_len]
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã visual –∏ text tokens
            # –≠—Ç–æ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ LLaVA
            # –û–±—ã—á–Ω–æ: [system_tokens] + [visual_tokens] + [question_tokens]
            
            # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ (–Ω—É–∂–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é LLaVA)
            visual_start = 5  # –ü–æ—Å–ª–µ system tokens
            visual_end = visual_start + 576  # 576 visual tokens
            text_start = visual_end
            text_end = text_start + question_length
            
            if visual_end < seq_len and text_end <= seq_len:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º attention –æ—Ç visual tokens –∫ text tokens
                visual_to_text = avg_attention[0, visual_start:visual_end, text_start:text_end]
                
                # –£—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ text tokens –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –∫–∞–∂–¥–æ–≥–æ visual token
                visual_importance = visual_to_text.mean(dim=1)  # [num_visual_tokens]
                
                processed[layer_name] = {
                    'visual_attention': visual_importance,
                    'visual_to_text': visual_to_text,
                    'full_attention': avg_attention[0]
                }
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–∑–Ω—ã—Ö —Å–ª–æ–µ–≤
        if processed:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π
            main_layer = list(processed.keys())[-1]
            main_attention = processed[main_layer]['visual_attention']
            
            return {
                'visual_attention': main_attention,
                'attention_scores': main_attention,
                'num_visual_tokens': len(main_attention),
                'layer_attention': processed,
                'is_mock': False
            }
        else:
            # Fallback –∫ mock –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫
            return self._mock_attention_extraction(None, "")


class VQAGroundTruthGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä ground truth –¥–∞–Ω–Ω—ã—Ö –¥–ª—è VQA fine-tuning"""
    
    def __init__(self, llava_model_path: str = "liuhaotian/llava-v1.5-7b", device: str = "cuda"):
        self.attention_extractor = LLaVAAttentionExtractor(llava_model_path, device)
        self.device = device
    
    def generate_vqa_ground_truth(
        self,
        image_path: str,
        question: str,
        answer: str = None
    ) -> Dict:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç ground truth –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ VQA –ø—Ä–∏–º–µ—Ä–∞
        
        Returns:
            Dictionary —Å visual features, question embeddings, target attention
        """
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        try:
            image = Image.open(image_path).convert('RGB')
        except Exception as e:
            print(f"‚ùå Error loading image {image_path}: {e}")
            return None
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ attention patterns
        attention_data = self.attention_extractor.extract_attention_weights(image, question)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ visual features (–∫–∞–∫ –≤ COCO dataset)
        from ..data.coco_pretraining import FeatureExtractor
        from ..config.base_config import ExperimentConfig
        
        config = ExperimentConfig()
        feature_extractor = FeatureExtractor(config)
        
        try:
            visual_features = feature_extractor.extract_visual_features(image)
            question_embeds = feature_extractor.extract_text_features(question)
        except Exception as e:
            print(f"‚ùå Error extracting features: {e}")
            return None
        
        return {
            'visual_features': visual_features,
            'question_embeds': question_embeds,
            'target_attention': attention_data['visual_attention'],
            'attention_scores': attention_data['attention_scores'],
            'question': question,
            'answer': answer,
            'image_path': image_path,
            'is_mock_attention': attention_data.get('is_mock', False)
        }
    
    def process_vqa_dataset(
        self,
        vqa_questions_file: str,
        vqa_annotations_file: str,
        coco_images_dir: str,
        output_file: str,
        max_samples: Optional[int] = None
    ):
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–µ—Å—å VQA dataset –∏ —Å–æ–∑–¥–∞–µ—Ç ground truth —Ñ–∞–π–ª
        """
        
        print(f"üîÑ Processing VQA dataset...")
        print(f"Questions: {vqa_questions_file}")
        print(f"Annotations: {vqa_annotations_file}")
        print(f"Images: {coco_images_dir}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º VQA –¥–∞–Ω–Ω—ã–µ
        with open(vqa_questions_file, 'r') as f:
            questions_data = json.load(f)
        
        if vqa_annotations_file:
            with open(vqa_annotations_file, 'r') as f:
                annotations_data = json.load(f)
            
            # –°–æ–∑–¥–∞–µ–º mapping question_id -> answer
            id_to_answer = {}
            for ann in annotations_data['annotations']:
                id_to_answer[ann['question_id']] = ann['multiple_choice_answer']
        else:
            id_to_answer = {}
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º samples
        processed_samples = []
        questions = questions_data['questions']
        
        if max_samples:
            questions = questions[:max_samples]
        
        for i, q_data in enumerate(questions):
            if i % 100 == 0:
                print(f"Processed {i}/{len(questions)} samples...")
            
            question_id = q_data['question_id']
            question = q_data['question']
            image_id = q_data['image_id']
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
            image_filename = f"COCO_val2014_{image_id:012d}.jpg"
            image_path = os.path.join(coco_images_dir, image_filename)
            
            if not os.path.exists(image_path):
                # –ü–æ–ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–æ–π —Ñ–æ—Ä–º–∞—Ç
                image_filename = f"{image_id:012d}.jpg"
                image_path = os.path.join(coco_images_dir, image_filename)
            
            if not os.path.exists(image_path):
                continue
            
            # –ü–æ–ª—É—á–∞–µ–º answer
            answer = id_to_answer.get(question_id, "")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ground truth
            gt_data = self.generate_vqa_ground_truth(image_path, question, answer)
            
            if gt_data:
                gt_data['question_id'] = question_id
                gt_data['image_id'] = image_id
                processed_samples.append(gt_data)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        print(f"üíæ Saving {len(processed_samples)} processed samples to {output_file}")
        torch.save(processed_samples, output_file)
        
        print(f"‚úÖ VQA ground truth generation completed!")
        return processed_samples


# Test –∏ utility functions
def test_attention_extraction():
    """–¢–µ—Å—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è attention"""
    
    extractor = LLaVAAttentionExtractor(device="cpu")
    
    # –°–æ–∑–¥–∞–µ–º test image
    test_image = Image.new('RGB', (336, 336), color='red')
    test_question = "What color is this image?"
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º attention
    attention_data = extractor.extract_attention_weights(test_image, test_question)
    
    print("‚úÖ Attention extraction test:")
    print(f"  Visual attention shape: {attention_data['visual_attention'].shape}")
    print(f"  Attention sum: {attention_data['visual_attention'].sum():.4f}")
    print(f"  Max attention: {attention_data['visual_attention'].max():.4f}")
    print(f"  Is mock: {attention_data.get('is_mock', False)}")
    
    return attention_data


if __name__ == "__main__":
    # –¢–µ—Å—Ç
    print("üß™ Testing LLaVA attention extraction...")
    test_attention_extraction()
    print("‚úÖ Test completed!")
